{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import ase.atoms\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "import config.paths as PATHS\n",
    "import src.features.features_extractors as features_extractors\n",
    "import src.features.input_parsers as input_parsers\n",
    "import src.visualization as visualization\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0xCAFFE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single particle loading for experiments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particle = ase.io.read(os.path.join('..', 'data', 'S_random_1.xyz'))\n",
    "# particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = comments_parser._load_lines_after_specified_one(\n",
    "#     os.path.join(\"..\", \"data\", \"S_random_8000.xyz\"),\n",
    "#     \"42\\n\"\n",
    "# )\n",
    "# y = pd.read_csv(os.path.join('..', 'data', 'S_random_8000.trans'), header=None)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sulfur_idxs = [10, 31]\n",
    "\n",
    "benzene1_idxs = [11, 14, 15, 16, 17, 20]\n",
    "benzene2_idxs = [21, 24, 25, 26, 27, 30]\n",
    "\n",
    "benzene1_plane_idxs = [14, 15, 16]\n",
    "benzene2_plane_idxs = [25, 26, 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to all particles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_parsers.read_raw_data(\n",
    "    PATHS.PARTICLES_FILE, PATHS.TRANSPORT_FILE, PATHS.FEATURES_CACHE\n",
    ")\n",
    "df[\"y\"] = np.log(df[\"y\"])  # ToDo: log dodany do y - refactor it\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extractors.add_benzene_dst_feature(df, benzene1_idxs, benzene2_idxs)\n",
    "features_extractors.add_benzene_cossq_feature(df, [11, 16, 17], [24, 27, 25])\n",
    "features_extractors.add_dst_feature(df, 32, 9)  # AuAu33_10\n",
    "features_extractors.add_dst_feature(df, 20, 21)  # CC21_22\n",
    "features_extractors.add_dst_feature(df, 31, 32)  # SAu32_33\n",
    "features_extractors.add_dst_feature(df, 32, 33)  # Au-Au (33-34)\n",
    "features_extractors.add_dst_feature(df, 30, 31)  # C-S (31-32)\n",
    "features_extractors.add_dst_feature(df, 27, 29)  # C-H (28-30)\n",
    "features_extractors.add_dst_feature(df, 15, 17)  # C-C (16-18)\n",
    "\n",
    "features_extractors.add_ang_feature(df, 32, 31, 30)  # Au_S_C_idxs_1\n",
    "features_extractors.add_ang_feature(df, 9, 10, 11)  # Au_S_C_idxs_2\n",
    "features_extractors.add_ang_feature(df, 33, 32, 31)  # Au_Au_S_idxs_1\n",
    "features_extractors.add_ang_feature(df, 32, 21, 9)  # Au_C_Au_idxs\n",
    "features_extractors.add_ang_feature(df, 20, 22, 23)  # H_C_H_idxs\n",
    "features_extractors.add_ang_feature(df, 21, 24, 26)  # C-C-C (np. 22-25-27)\n",
    "features_extractors.add_ang_feature(df, 24, 25, 27)  # H-C-C (np. 24-26-28)\n",
    "features_extractors.add_ang_feature(df, 27, 30, 31)  # C-C-S (np. 28-31-32)\n",
    "features_extractors.add_ang_feature(df, 31, 33, 35)  # Au-Au-Au (33-34-36)\n",
    "\n",
    "# kąt torsyjny Au-S-C-C (np. 33-32-31-27)\n",
    "features_extractors.add_dih_feature(df, 32, 31, 30, 26)\n",
    "# kąt torsyjny Au-Au-S-C (np. 8-10-11-12)\n",
    "features_extractors.add_dih_feature(df, 7, 9, 10, 11)\n",
    "\n",
    "comments_df = input_parsers.get_comments_df(PATHS.PARTICLES_FILE)\n",
    "df[\"fermi_energy\"] = comments_df[\"fermi_energy\"]\n",
    "df[\"total_energy\"] = comments_df[\"total_energy\"]\n",
    "df[\"HOMO_LUMO_diff\"] = comments_df[\"energy_level_130\"] - comments_df[\"energy_level_131\"]\n",
    "# df[\"electron_state_143\"] = comments_df[\"electron_state_143\"]\n",
    "# df[\"electron_state_144\"] = comments_df[\"electron_state_144\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: zmienic korelację na rank - tao-kendala rosse-...\n",
    "\n",
    "corr_matrix = df.drop(\"obj\", axis=1).corr(numeric_only=False)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 15)\n",
    "visualization.draw_correlation_matrix(corr_matrix, ax)\n",
    "plt.show()\n",
    "#  ToDo: cossingus kąta benzenu do kwadratu jako predyktor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_treshold = 0.00\n",
    "restricted_features = corr_matrix[corr_matrix[\"y\"].abs() >= corr_treshold].index[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"y\"]\n",
    "X = df[restricted_features]\n",
    "\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, test_size=0.25, random_state=seed\n",
    ")\n",
    "for x in [X_train, X_test, y_train, y_test]:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.ensemble.GradientBoostingRegressor(\n",
    "    # learning_rate=0.25, max_depth=3, n_estimators=50, random_state=seed\n",
    "    **dict(\n",
    "        {\n",
    "            \"ccp_alpha\": 0,\n",
    "            \"learning_rate\": 0.2,  # 0.25\n",
    "            \"max_depth\": 4,\n",
    "            \"max_features\": \"sqrt\",\n",
    "            \"n_estimators\": 30,\n",
    "            \"random_state\": seed,\n",
    "            \"subsample\": 0.8,  # 1\n",
    "        }\n",
    "    )\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "eval_y_test, eval_y_pred = np.exp(y_test), np.exp(y_pred)\n",
    "eval_y_train, eval_y_train_pred = np.exp(y_train), np.exp(y_train_pred)\n",
    "\n",
    "mse = sklearn.metrics.mean_squared_error(eval_y_test, eval_y_pred)\n",
    "rmse = sklearn.metrics.mean_squared_error(eval_y_test, eval_y_pred, squared=False)\n",
    "mae = sklearn.metrics.mean_absolute_error(eval_y_test, eval_y_pred)\n",
    "mape = sklearn.metrics.mean_absolute_percentage_error(eval_y_test, eval_y_pred)\n",
    "\n",
    "train_mse = sklearn.metrics.mean_squared_error(eval_y_train, eval_y_train_pred)\n",
    "train_rmse = sklearn.metrics.mean_squared_error(\n",
    "    eval_y_train, eval_y_train_pred, squared=False\n",
    ")\n",
    "train_mae = sklearn.metrics.mean_absolute_error(eval_y_train, eval_y_train_pred)\n",
    "train_mape = sklearn.metrics.mean_absolute_percentage_error(\n",
    "    eval_y_train, eval_y_train_pred\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Metric \\tTest \\tTrain\",\n",
    "    \"#\" * 22,\n",
    "    \"MSE:\\t{:6.4f}\\t{:6.4f}\".format(mse, train_mse),\n",
    "    \"RMSE:\\t{:6.4f}\\t{:6.4f}\".format(rmse, train_rmse),\n",
    "    \"MAE:\\t{:6.4f}\\t{:6.4f}\".format(mae, train_mae),\n",
    "    \"MAPE:\\t{:6.4f}\\t{:6.4f}\".format(mape, train_mape),\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "# MAPE:\t0.4837\t0.3913 default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(range(len(y_test)), sorted(list(y_test - y_pred)))\n",
    "results_df = pd.DataFrame(\n",
    "    {\"y_test\": eval_y_test, \"y_pred\": eval_y_pred, \"diff\": eval_y_test - eval_y_pred}\n",
    ")\n",
    "print(results_df[\"diff\"].abs().std())\n",
    "print(y_pred.std(), y_test.std())\n",
    "results_df.sort_values(\"y_test\")[\"diff\"].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(\"y_test\")[[\"y_test\", \"y_pred\"]].plot.scatter(\"y_test\", \"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_correlations = (\n",
    "    corr_matrix.loc[\"y\", corr_matrix[\"y\"].abs() >= corr_treshold]\n",
    "    .abs()\n",
    "    .rename(\"abs_corelations\")\n",
    ")\n",
    "features_importance = pd.Series(\n",
    "    clf.feature_importances_, index=restricted_features\n",
    ").rename(\"feature_importance\")\n",
    "\n",
    "importance_df = pd.merge(\n",
    "    features_importance, abs_correlations, left_index=True, right_index=True\n",
    ")\n",
    "importance_df.sort_values(\"feature_importance\").plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for complex analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# def calculate_mape(y_true, y_pred):\n",
    "#     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "# mape_scorer = make_scorer(calculate_mape, greater_is_better=False)\n",
    "\n",
    "# # create a GradientBoostingRegressor object\n",
    "# clf = sklearn.ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# # define the hyperparameter grid to search over\n",
    "# param_grid = {\n",
    "#     \"learning_rate\": [0.3, 0.25, 0.2],\n",
    "#     \"n_estimators\": [30],\n",
    "#     \"max_depth\": [3, 4],\n",
    "#     \"subsample\": [0.75, 1],\n",
    "#     \"n_iter_no_change\": [3, None],\n",
    "#     \"ccp_alpha\": [0, 0.001, 0.1, 0.5],\n",
    "#     \"max_features\": [1.0, \"sqrt\", \"log2\", None],\n",
    "#     \"random_state\": [seed],\n",
    "# }\n",
    "\n",
    "# # define the grid search object\n",
    "# grid_search = GridSearchCV(\n",
    "#     clf,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,\n",
    "#     scoring=mape_scorer,  #'neg_mean_squared_error',\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "\n",
    "# # fit the grid search object to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # print the best hyperparameters and the corresponding mean squared error\n",
    "# print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "# print(\"Best MSE: \", abs(grid_search.best_score_))\n",
    "\n",
    "# # predict on the test data using the best model\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# eval_y_test, eval_y_pred = np.exp(y_test), np.exp(y_pred)\n",
    "\n",
    "# mse = sklearn.metrics.mean_squared_error(eval_y_test, eval_y_pred)\n",
    "# rmse = sklearn.metrics.mean_squared_error(eval_y_test, eval_y_pred, squared=False)\n",
    "# mae = sklearn.metrics.mean_absolute_error(eval_y_test, eval_y_pred)\n",
    "# calculate_mape = sklearn.metrics.mean_absolute_percentage_error(\n",
    "#     eval_y_test, eval_y_pred\n",
    "# )\n",
    "\n",
    "# print(\"\\nMSE:\", mse, \"\\nRMSE:\", rmse, \"\\nMAE:\", mae, \"\\nMAPE:\", calculate_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done: wziąc te obsadzenia stanów które nie są jedynkami ani zerami\n",
    "# nie pomogły\n",
    "# Done: Osobne modele na danych elektornicznych i danych fizycznych\n",
    "# same fizyczne dane straciły mniej niż procent na testowym i 3 na treningowym\n",
    "# same elektoniczne starciły 8% na testowym, ale to w sumie 3 ficzery + obsadzenia stanów\n",
    "\n",
    "# shap <- do istotnosci ficzerów\n",
    "# DScribe:\n",
    "# Edwald sum\n",
    "# MBTR\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
